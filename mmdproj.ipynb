{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3fwpBdo9zqF",
        "outputId": "1a315462-dad1-4932-a6b3-48841d7bafd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/12/12 10:31:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/12/12 10:31:27 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
            "24/12/12 10:31:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        " from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, struct, collect_list, udf\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, IntegerType, DoubleType\n",
        "\n",
        "# Step 1: Initialize SparkSession with updated configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AmazonReview\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.executor.memory\", \"24g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
        "    .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
        "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.executor.extraJavaOptions\", \"-Xss4m\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-Xss4m\") \\\n",
        "    .config(\"spark.local.dir\", \"/scratch/szele/\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setCheckpointDir(\"/scratch/szele/tmp/spark_checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR3UTpj69zqH",
        "outputId": "c04421f0-f815-44cf-b12d-38706e4e11e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null value counts:\n",
            " class_index      0\n",
            "review_title    77\n",
            "review_text      7\n",
            "dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 317\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import concat_ws, regexp_replace, lower, when, col\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Path to your CSV file\n",
        "csv_file_path = \"train.csv\"\n",
        "\n",
        "# Read the CSV file\n",
        "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Check for null values\n",
        "\n",
        "null_counts = df.select([col(c).isNull().alias(c) for c in df.columns]).toPandas().sum()\n",
        "print(\"Null value counts:\\n\", null_counts)\n",
        "\n",
        "# Drop rows with null values in review_title or review_text\n",
        "df = df.dropna(subset=[\"review_title\", \"review_text\"])\n",
        "\n",
        "\n",
        "duplicate_count = df.count() - df.dropDuplicates().count()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "df = df.dropDuplicates()\n",
        "\n",
        "# Replace class_index 2 with 0 for binary classification\n",
        "df = df.withColumn(\"class_index\", when(col(\"class_index\") == 2, 0).otherwise(col(\"class_index\")))\n",
        "\n",
        "# Combine review_title and review_text into a single column\n",
        "df = df.withColumn(\"combined_text\", concat_ws(\" \", col(\"review_title\"), col(\"review_text\")))\n",
        "\n",
        "# Convert text to lowercase\n",
        "df = df.withColumn(\"cleaned_text\", lower(col(\"combined_text\")))\n",
        "\n",
        "# Remove special characters and numbers, keeping only letters and spaces\n",
        "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(\"review_title\", \"review_text\", \"combined_text\")\n",
        "\n",
        "# Tokenize the cleaned text into words\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
        "df = tokenizer.transform(df)\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"text_without_stopwords\")\n",
        "df = remover.transform(df)\n",
        "\n",
        "# Define a UDF to filter out empty strings from the list\n",
        "def remove_empty_strings(token_list):\n",
        "    return [token for token in token_list if token.strip() != \"\" and len(token) >= 3]\n",
        "\n",
        "remove_empty_udf = udf(remove_empty_strings, ArrayType(StringType()))\n",
        "\n",
        "# Apply the UDF to clean the tokens\n",
        "df = df.withColumn(\"text_without_stopwords\", remove_empty_udf(col(\"text_without_stopwords\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQKveIzG9zqH",
        "outputId": "8f6dacea-0d24-4b39-9e82-e918f59148d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null value counts:\n",
            " class_index      0\n",
            "review_title    10\n",
            "review_text      0\n",
            "dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tfitestdf_features` cannot be resolved. Did you mean one of the following? [`tfidf_features`, `cleaned_text`, `term_frequency`, `class_index`, `tokens`].;\n'Project [text_without_stopwords#386, 'tfitestdf_features]\n+- Project [class_index#341, cleaned_text#356, tokens#367, text_without_stopwords#386, term_frequency#393, UDF(term_frequency#393) AS tfidf_features#408]\n   +- Project [class_index#341, cleaned_text#356, tokens#367, text_without_stopwords#386, UDF(text_without_stopwords#386) AS term_frequency#393]\n      +- Project [class_index#341, cleaned_text#356, tokens#367, remove_empty_strings(text_without_stopwords#377)#385 AS text_without_stopwords#386]\n         +- Project [class_index#341, cleaned_text#356, tokens#367, UDF(tokens#367) AS text_without_stopwords#377]\n            +- Project [class_index#341, cleaned_text#356, UDF(cleaned_text#356) AS tokens#367]\n               +- Project [class_index#341, cleaned_text#356]\n                  +- Project [class_index#341, review_title#302, review_text#303, combined_text#345, regexp_replace(cleaned_text#350, [^a-zA-Z\\s], , 1) AS cleaned_text#356]\n                     +- Project [class_index#341, review_title#302, review_text#303, combined_text#345, lower(combined_text#345) AS cleaned_text#350]\n                        +- Project [class_index#341, review_title#302, review_text#303, concat_ws( , review_title#302, review_text#303) AS combined_text#345]\n                           +- Project [CASE WHEN (class_index#301 = 2) THEN 0 ELSE class_index#301 END AS class_index#341, review_title#302, review_text#303]\n                              +- Deduplicate [class_index#301, review_title#302, review_text#303]\n                                 +- Filter atleastnnonnulls(2, review_title#302, review_text#303)\n                                    +- Relation [class_index#301,review_title#302,review_text#303] csv\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m testdf \u001b[38;5;241m=\u001b[39m test_idf_model\u001b[38;5;241m.\u001b[39mtransform(testdf)  \u001b[38;5;66;03m# Transform the data using the fitted model\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Display the resulting DataFrame with TF-Itestdf features\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtestdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_without_stopwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtfitestdf_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HashingTF, IDF, PCA, ChiSqSelector\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lit\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m \n\u001b[1;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tfitestdf_features` cannot be resolved. Did you mean one of the following? [`tfidf_features`, `cleaned_text`, `term_frequency`, `class_index`, `tokens`].;\n'Project [text_without_stopwords#386, 'tfitestdf_features]\n+- Project [class_index#341, cleaned_text#356, tokens#367, text_without_stopwords#386, term_frequency#393, UDF(term_frequency#393) AS tfidf_features#408]\n   +- Project [class_index#341, cleaned_text#356, tokens#367, text_without_stopwords#386, UDF(text_without_stopwords#386) AS term_frequency#393]\n      +- Project [class_index#341, cleaned_text#356, tokens#367, remove_empty_strings(text_without_stopwords#377)#385 AS text_without_stopwords#386]\n         +- Project [class_index#341, cleaned_text#356, tokens#367, UDF(tokens#367) AS text_without_stopwords#377]\n            +- Project [class_index#341, cleaned_text#356, UDF(cleaned_text#356) AS tokens#367]\n               +- Project [class_index#341, cleaned_text#356]\n                  +- Project [class_index#341, review_title#302, review_text#303, combined_text#345, regexp_replace(cleaned_text#350, [^a-zA-Z\\s], , 1) AS cleaned_text#356]\n                     +- Project [class_index#341, review_title#302, review_text#303, combined_text#345, lower(combined_text#345) AS cleaned_text#350]\n                        +- Project [class_index#341, review_title#302, review_text#303, concat_ws( , review_title#302, review_text#303) AS combined_text#345]\n                           +- Project [CASE WHEN (class_index#301 = 2) THEN 0 ELSE class_index#301 END AS class_index#341, review_title#302, review_text#303]\n                              +- Deduplicate [class_index#301, review_title#302, review_text#303]\n                                 +- Filter atleastnnonnulls(2, review_title#302, review_text#303)\n                                    +- Relation [class_index#301,review_title#302,review_text#303] csv\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import concat_ws, regexp_replace, lower, when, col\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Path to your CSV file\n",
        "test_csv_file_path = \"test.csv\"\n",
        "\n",
        "# Read the CSV file\n",
        "testdf = spark.read.csv(test_csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Check for null values\n",
        "\n",
        "null_counts = testdf.select([col(c).isNull().alias(c) for c in testdf.columns]).toPandas().sum()\n",
        "print(\"Null value counts:\\n\", null_counts)\n",
        "\n",
        "# Drop rows with null values in review_title or review_text\n",
        "testdf = testdf.dropna(subset=[\"review_title\", \"review_text\"])\n",
        "\n",
        "\n",
        "duplicate_count = testdf.count() - testdf.dropDuplicates().count()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "testdf = testdf.dropDuplicates()\n",
        "\n",
        "# Replace class_index 2 with 0 for binary classification\n",
        "testdf = testdf.withColumn(\"class_index\", when(col(\"class_index\") == 2, 0).otherwise(col(\"class_index\")))\n",
        "\n",
        "# Combine review_title and review_text into a single column\n",
        "testdf = testdf.withColumn(\"combined_text\", concat_ws(\" \", col(\"review_title\"), col(\"review_text\")))\n",
        "\n",
        "# Convert text to lowercase\n",
        "testdf = testdf.withColumn(\"cleaned_text\", lower(col(\"combined_text\")))\n",
        "\n",
        "# Remove special characters and numbers, keeping only letters and spaces\n",
        "testdf = testdf.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "\n",
        "# Drop unnecessary columns\n",
        "testdf = testdf.drop(\"review_title\", \"review_text\", \"combined_text\")\n",
        "\n",
        "# Tokenize the cleaned text into words\n",
        "test_tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
        "testdf = test_tokenizer.transform(testdf)\n",
        "\n",
        "# Remove stop words\n",
        "test_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"text_without_stopwords\")\n",
        "testdf = test_remover.transform(testdf)\n",
        "\n",
        "# Define a Utestdf to filter out empty strings from the list\n",
        "def remove_empty_strings(token_list):\n",
        "    return [token for token in token_list if token.strip() != \"\" and len(token) >= 3]\n",
        "\n",
        "remove_empty_testdf = udf(remove_empty_strings, ArrayType(StringType()))\n",
        "\n",
        "# Apply the Utestdf to clean the tokens\n",
        "testdf = testdf.withColumn(\"text_without_stopwords\", remove_empty_testdf(col(\"text_without_stopwords\")))\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "\n",
        "\n",
        "# Apply HashingTF to convert tokens into term frequency vectors\n",
        "testhashingTF = HashingTF(inputCol=\"text_without_stopwords\", outputCol=\"term_frequency\", numFeatures=10000)\n",
        "testdf = testhashingTF.transform(testdf)\n",
        "\n",
        "# Apply IDF to compute the TF-IDF scores\n",
        "test_idf = IDF(inputCol=\"term_frequency\", outputCol=\"tfidf_features\")\n",
        "test_idf_model = test_idf.fit(testdf)  # Fit the IDF model\n",
        "testdf = test_idf_model.transform(testdf)  # Transform the data using the fitted model\n",
        "\n",
        "\n",
        "# Display the resulting DataFrame with TF-Itestdf features\n",
        "testdf.select(\"text_without_stopwords\", \"tfitestdf_features\").show(10, truncate=False)\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, PCA, ChiSqSelector\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Option 1: Use PCA for dimensionality reduction\n",
        "testpca = PCA(k=100, inputCol=\"tfitestdf_features\", outputCol=\"reduced_features\")  # Reduce to 50 dimensions\n",
        "test_pca_model = testpca.fit(testdf)\n",
        "testdf = test_pca_model.transform(testdf)\n",
        "\n",
        "from pyspark.ml.linalg import Vectors,VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "\n",
        "# Define the number of PCA components (k)\n",
        "k = 100  # Ensure this matches your PCA configuration\n",
        "\n",
        "# UDF to pad or truncate the vector to length k\n",
        "def pad_or_truncate(vector, length):\n",
        "    if vector.size < length:\n",
        "        # Pad with zeros if the vector is smaller than k\n",
        "        return Vectors.dense(list(vector.toArray()) + [0.0] * (length - vector.size))\n",
        "    elif vector.size > length:\n",
        "        # Truncate if the vector is larger than k (shouldn't happen in PCA)\n",
        "        return Vectors.dense(vector.toArray()[:length])\n",
        "    return vector\n",
        "\n",
        "test_pad_or_truncate_udf = udf(lambda x: pad_or_truncate(x, k), VectorUDT())\n",
        "\n",
        "# Convert PCA output to fixed-length dense vector\n",
        "testdf = testdf.withColumn(\"reduced_features\", test_pad_or_truncate_udf(col(\"reduced_features\")))\n",
        "\n",
        "# Drop unnecessary columns explicitly\n",
        "columns_to_drop = [\"cleaned_text\", \"tokens\", \"text_without_stopwords\", \"term_frequency\",\"tfidf_features\"]\n",
        "for col in columns_to_drop:\n",
        "    testdf = testdf.drop(col)\n",
        "\n",
        "# Verify the schema\n",
        "testdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjjQ158M9zqI",
        "outputId": "81363d4c-d303-4a20-818e-e32a6c370475"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of rows: 3599599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique values in 'class_index': 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 373:===================>                                   (6 + 11) / 17]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+\n",
            "|class_index|  count|\n",
            "+-----------+-------+\n",
            "|          0|1799904|\n",
            "|          1|1799695|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# # Total row count\n",
        "total_count = df.count()\n",
        "print(f\"Total number of rows: {total_count}\")\n",
        "\n",
        "# # Count of unique values in class_index\n",
        "unique_class_count = df.select(\"class_index\").distinct().count()\n",
        "print(f\"Number of unique values in 'class_index': {unique_class_count}\")\n",
        "\n",
        "# # Count occurrences of each unique value in class_index\n",
        "class_counts = df.groupBy(\"class_index\").count()\n",
        "class_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMmenyrD9zqI",
        "outputId": "c77aa1c0-74a1-4e80-c902-4de4a739a86c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 16:================================>                        (9 + 7) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text_without_stopwords                                                                                                                                                                                                                                                                                                                                                                                         |tfidf_features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[great, classic, price, first, time, reading, classic, recommend, love, may, want, nicer, version, bookshelf, nice, quality, book, though, printing, isnt, small, neither, book, fits, right, paperback, bookshelf, perfectly, others, like, itand, story, many, pages, searches, reviews, classic, outlive, lot, say, human, beings, culture, past, futureone, best, classics, great, one, get, started, read]|(10000,[222,393,566,694,750,763,785,815,979,1738,1790,3014,3165,3303,3330,3370,3385,3506,4099,4116,4230,4266,4526,4943,5065,5188,5762,5862,6240,6403,6855,6916,6940,7158,7547,7650,7712,7927,7930,8157,8303,8404,8552,8645,9160,9275,9426],[3.4078507587347704,2.8357216671344543,7.134265977388264,5.894327550955805,3.140818559331162,2.424704386293301,4.927064237607359,1.3721302293987152,5.32073592491172,3.0726702012779383,3.1616560635008564,10.20738164458982,6.5005137742705035,3.7949563552671863,1.6093295729313282,3.105485626681671,3.277785148037459,2.900932381618667,4.24837579146634,2.7394899522396408,6.885665254978954,3.993438334902798,2.9350279074942964,3.0684786285729744,5.235440131238223,2.6075810597827673,5.919342893847302,2.903654945139984,2.3214784216373356,2.1991104527149945,2.7219830739388073,3.6370195906203513,3.741852707921984,11.907122671954495,7.885514832669711,2.020958796180465,2.7707058421211905,4.612055710438973,3.710923061472505,1.9706314874722022,4.77170414611263,7.457172114482758,3.2742847583913597,5.117970198960696,3.453792837675387,4.007352632026737,1.920656655840585])|\n",
            "|[adapter, shown, one, shipped, received, different, adapter, really, tell, macally, adapter, received, adapter, inspire, longer, cord, velcro, strap, led, lights, charge, computer, arrived, quickly]                                                                                                                                                                                                         |(10000,[505,815,1850,2369,2987,3190,3391,3601,4112,5304,5659,5870,6683,6812,7211,8151,8351,8599,8802,9711],[7.7049180514665725,1.3721302293987152,3.785928003658853,4.273938427948494,5.540136277662895,4.885434565973026,4.25946192430547,7.317254359835083,2.117713266438541,5.396615671205628,3.7439052658356413,5.5824867063670816,3.2423816836008785,4.952923731052775,4.2171546584502,4.311312091689652,5.121781675174798,22.72600900540299,6.43313680060585,4.652354517184062])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|[fit, kia, optima, purchased, kia, optima, went, install, everything, seemed, good, car, inch, wider, hitch, hitch, centered, car, either, hitch, center, cars, mounts, otherwise, seemed, like, great, hitch]                                                                                                                                                                                                 |(10000,[750,814,1480,1499,2145,2162,2380,3330,3674,3866,5402,5884,6168,7536,8348,9126,9156,9371,9460,9667,9690],[1.570409279665581,5.432309233787109,4.0126534579452695,9.027949955929332,4.735705737259616,4.003494302485217,28.105460548328356,1.6093295729313282,8.321138271145868,7.406504617405447,3.254024940469597,12.890967419826039,1.5918757869526776,6.77515497865165,3.8530928169218286,3.975932025147066,3.6339014353886885,3.7250520339067754,8.695844236160127,5.026611215464779,5.631273179416794])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|[youll, love, saint, martin, although, may, written, childs, book]                                                                                                                                                                                                                                                                                                                                             |(10000,[393,1174,1727,1927,4546,4989,6240,7779,9160],[1.4178608335672271,3.3518279558367396,5.620556450661287,5.39215069544017,4.138864769083287,6.22679450894819,2.3214784216373356,2.4263694040349804,3.453792837675387])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|[complete, piece, junk, one, kid, fun, bought, one, gift, son, rocke, didnt, mate, correctly, pump, tried, gamely, hold, place, pumped, times, point, valve, pump, separated, body, virtually, force, applied, part, pump, press, fit, piece, couldnt, possibly, withstood, real, pressure, pump, total, joke, complicated, piece, equipment, never, built, anything, dupe, consumer]                          |(10000,[50,209,281,391,726,815,841,992,1523,1998,2034,2114,2303,2389,2785,3133,3267,3723,4111,4192,4319,4337,4376,4416,4646,5055,5150,5661,6279,6390,6538,7704,8001,8140,8305,8348,8358,8740,9030,9475,9691,9706,9837,9877],[5.874855169755292,4.053283458608739,2.672193954797855,4.826086804379718,5.770078289569392,2.7442604587974304,4.7891488545602865,4.12133061282206,4.7724920130038875,5.072222291282063,3.8926132004399534,3.464993873471074,5.371972399503238,4.129463464843567,6.008499588658545,11.571994921797636,5.612384411445326,4.926451140297278,7.483008306601291,5.573740187779762,3.1622268566235348,6.05204749849583,4.951626535564363,3.855124685169327,2.3964702967822507,6.371963336933562,3.4933660404970435,5.473883263338915,4.676256749950707,3.87965984430655,2.7456885464577545,6.655805179661178,3.6631114196825836,23.57207011641085,3.197621747997568,3.8530928169218286,4.037506605694146,3.5421411255456428,7.439996119711748,6.1945581975339525,3.257700461341859,4.751530972603941,7.631250549742383,6.0818861509897975])                                                                           |\n",
            "|[works, greatshort, comings, yes, retired, negative, wedding, photographer, want, spoton, results, digital, age, trouble, getting, exposurewhite, balance, correct, yes, processor, correct, slight, errors, appreciate, important, white, balance]                                                                                                                                                            |(10000,[312,677,1140,1967,2540,2763,2909,3060,3173,3430,4182,4553,5044,5843,6486,6536,7353,7501,7712,7921,8044,8756,8791,9987],[9.321437212839793,5.018430348382945,10.136041766320746,4.556163090026984,3.581260129114243,6.07570168939372,4.780040960194253,4.976001984440121,5.985923752790797,7.17770463279969,5.062607256682074,6.229751632837933,8.71327542041041,4.4360666928010835,7.8906981097315665,5.370000660448398,3.0949075641348256,4.4641383776473065,2.7707058421211905,7.906411115396123,6.518797865719532,6.817651123171025,4.585256441407504,5.833400464046577])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
            "|[good, purchases, set, simple, craft, projectsmainly, cut, card, stock, cardboard, tried, hammering, punches, hammer, didnt, card, stock, got, husbands, big, beefy, hammerit, still, punch, thin, card, stock, disappointed, product, glad, didnt, pay, lotcomplete, waste]                                                                                                                                   |(10000,[55,157,447,793,1203,1206,1283,2144,2167,2630,2665,2913,3048,3712,4012,4328,4897,5150,5240,5342,5516,6010,6145,6168,6538,6576,8133,8382,9179],[2.941548671855263,2.0815484379253837,2.4398922776678336,15.530031457332656,7.367477462289387,4.4440510022070425,6.32713701473093,12.499713569989723,4.911999472163304,3.1967178683898716,4.397525612311312,2.9625679272808276,3.311763671417694,2.707185201391915,3.183431079966679,8.425566965296056,4.21489562134492,3.4933660404970435,6.5343579519538,3.2980860319982925,5.886693159280141,2.617024915967172,6.169019175081335,1.5918757869526776,5.491377092915509,5.899584867685215,5.463146408226809,6.110637591606225,5.112741489364141])                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "|[umbra, tension, rod, pleased, umbra, tension, rod, needed, rod, hardware, new, house, concrete, block, walls, deep, window, wells, tension, rod, fits, bill, attractive]                                                                                                                                                                                                                                      |(10000,[171,585,785,1000,1762,2162,2403,4380,4548,4782,5490,5836,8135,8454,9246,9755,9962],[6.385054671011496,2.5761508763246974,4.927064237607359,15.305589257681795,4.90372719336806,4.003494302485217,5.686158932287162,6.0036509576339805,5.238156956315805,26.273591992775163,5.940449099961807,4.205630663394949,5.556401438636757,5.975917621693735,3.957490548051198,8.395376739959492,5.6358569577879365])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|[beautiful, glasses, great, price, attractive, looking, glasses, feel, nice, well, made, flimsy, yet, heavy, hold, make, whatever, drinking, seem, treat]                                                                                                                                                                                                                                                      |(10000,[157,750,1273,1738,2389,2588,3115,3344,3370,3525,4037,4232,4551,5490,7695,8750,8949,9066,9216],[2.0815484379253837,1.570409279665581,3.769279672810678,3.0726702012779383,4.129463464843567,3.9011368149757795,3.3815859703613675,2.739782410265397,3.105485626681671,2.6546669153603757,3.481609643110666,3.1454819766416016,12.210786958402247,5.940449099961807,5.1990650334197195,5.67193047158773,4.458244758153497,4.094150115076388,4.459973159572807])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|[end, haiku, know, become, haiku, english, bad, haiku, america, general, vastly, misunderstood, verses, volume, show, three, lines, kigo, looks, one, deduce, poets, gleamed, knowledge, haiku, kerouac, another, haiku, fraud, rather, basho, experiment, really]                                                                                                                                             |(10000,[440,453,759,815,1404,1716,1868,2365,2837,3199,3395,3413,4019,4069,4112,5285,5329,5465,5925,6078,7090,7779,7864,7956,8166,8848,9059,9658,9661],[4.79511042355958,3.34149747296609,4.86249936471458,1.3721302293987152,4.502151559206924,4.056183288413456,7.054920376748883,6.795059800950583,3.978185246575026,3.7840562946585408,7.049463775182354,5.051910566684311,3.6101112040062517,5.468401626259743,2.117713266438541,7.049463775182354,5.170155123686104,4.82369888817175,3.6756546976841453,7.133569355973815,4.390059702528468,2.4263694040349804,4.882167608859761,2.781482471789428,2.887853540004334,39.1092686368403,7.509022780119315,6.835581931441418,3.7774417002143483])                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Apply HashingTF to convert tokens into term frequency vectors\n",
        "hashingTF = HashingTF(inputCol=\"text_without_stopwords\", outputCol=\"term_frequency\", numFeatures=10000)\n",
        "df = hashingTF.transform(df)\n",
        "\n",
        "# Apply IDF to compute the TF-IDF scores\n",
        "idf = IDF(inputCol=\"term_frequency\", outputCol=\"tfidf_features\")\n",
        "idfModel = idf.fit(df)\n",
        "df = idfModel.transform(df)\n",
        "\n",
        "# Display the resulting DataFrame with TF-IDF features\n",
        "df.select(\"text_without_stopwords\", \"tfidf_features\").show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sayKXn3C9zqI",
        "outputId": "bc42b699-ee8d-4a2f-f54a-73c1533bfbbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/12 10:42:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, PCA, ChiSqSelector\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Option 1: Use PCA for dimensionality reduction\n",
        "pca = PCA(k=100, inputCol=\"tfidf_features\", outputCol=\"reduced_features\")  # Reduce to 50 dimensions\n",
        "pca_model = pca.fit(df)\n",
        "df = pca_model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suwbAvcf9zqI"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors,VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "\n",
        "# Define the number of PCA components (k)\n",
        "k = 100  # Ensure this matches your PCA configuration\n",
        "\n",
        "# UDF to pad or truncate the vector to length k\n",
        "def pad_or_truncate(vector, length):\n",
        "    if vector.size < length:\n",
        "        # Pad with zeros if the vector is smaller than k\n",
        "        return Vectors.dense(list(vector.toArray()) + [0.0] * (length - vector.size))\n",
        "    elif vector.size > length:\n",
        "        # Truncate if the vector is larger than k (shouldn't happen in PCA)\n",
        "        return Vectors.dense(vector.toArray()[:length])\n",
        "    return vector\n",
        "\n",
        "pad_or_truncate_udf = udf(lambda x: pad_or_truncate(x, k), VectorUDT())\n",
        "\n",
        "# Convert PCA output to fixed-length dense vector\n",
        "df = df.withColumn(\"reduced_features\", pad_or_truncate_udf(col(\"reduced_features\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIaYQY2C9zqI",
        "outputId": "f8a11568-e912-4731-bbcb-f54367b78c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- class_index: integer (nullable = true)\n",
            " |-- reduced_features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Drop unnecessary columns explicitly\n",
        "columns_to_drop = [\"cleaned_text\", \"tokens\", \"text_without_stopwords\", \"term_frequency\",\"tfidf_features\"]\n",
        "for col in columns_to_drop:\n",
        "    df = df.drop(col)\n",
        "\n",
        "# Verify the schema\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE9m8lnc9zqJ"
      },
      "outputs": [],
      "source": [
        "# Verify the schema\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t-K9BZX9zqJ"
      },
      "outputs": [],
      "source": [
        "labeled_fraction = 0.5\n",
        "# Split data into labeled and unlabeled datasets\n",
        "labeled_data = df.sample(withReplacement=False, fraction=labeled_fraction, seed=42).cache()\n",
        "unlabeled_data = df.subtract(labeled_data).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws2sVA9H9zqJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Parameters\n",
        "initial_temperature = 1.0\n",
        "cooling_rate = 0.9\n",
        "\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(predictions):\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"class_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"class_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "    evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"class_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "    precision = evaluator_precision.evaluate(predictions)\n",
        "    recall = evaluator_recall.evaluate(predictions)\n",
        "\n",
        "    return accuracy, precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nezZuVq9zqJ"
      },
      "outputs": [],
      "source": [
        "# Train initial Random Forest on labeled data\n",
        "rf = RandomForestClassifier(featuresCol=\"reduced_features\", labelCol=\"class_index\", numTrees=100)\n",
        "rf_model = rf.fit(labeled_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE4zY6-9zqJ"
      },
      "source": [
        "single iteration implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3hnc_aY9zqK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# First iteration of iterative training\n",
        "print(\"\\n--- Iteration 1 ---\")\n",
        "\n",
        "# Step 4.1: Assign pseudo-labels to unlabeled data\n",
        "pseudo_labeled_data = rf_model.transform(unlabeled_data).select(\"reduced_features\", \"prediction\")\n",
        "\n",
        "# Rename and cast columns to match the labeled data schema\n",
        "pseudo_labeled_data = (\n",
        "    pseudo_labeled_data\n",
        "    .withColumnRenamed(\"prediction\", \"class_index\")            # Rename 'prediction' to 'class_index'\n",
        "    .withColumn(\"class_index\", col(\"class_index\").cast(\"int\"))  # Ensure class_index is INT\n",
        ")\n",
        "\n",
        "# Reorder columns in pseudo_labeled_data to match labeled_data\n",
        "pseudo_labeled_data = pseudo_labeled_data.select(\"class_index\", \"reduced_features\")\n",
        "\n",
        "# Cache pseudo-labeled data\n",
        "pseudo_labeled_data = pseudo_labeled_data.cache()\n",
        "pseudo_labeled_data.count()  # Trigger caching\n",
        "\n",
        "# Step 4.2: Combine labeled and pseudo-labeled data\n",
        "# Ensure schemas of both datasets are identical\n",
        "combined_data = labeled_data.union(pseudo_labeled_data).checkpoint()\n",
        "\n",
        "# Step 4.3: Train new Random Forest with combined data\n",
        "rf_model = rf.fit(combined_data)\n",
        "\n",
        "# Step 4.4: Evaluate Out-Of-Bag Error (OOBE) on labeled data\n",
        "predictions = rf_model.transform(labeled_data)\n",
        "accuracy, precision, recall = calculate_metrics(predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "# Step 4.5: Reduce temperature (if using simulated annealing or similar strategy)\n",
        "temperature = initial_temperature * cooling_rate\n",
        "\n",
        "# Unpersist pseudo-labeled data\n",
        "pseudo_labeled_data.unpersist()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLFMHQ_9zqK"
      },
      "source": [
        "experiment 1: 50 percent split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXL9U1Y39zqK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Parameters for iterative training\n",
        "num_iterations = 10\n",
        "initial_temperature = 1.0\n",
        "cooling_rate = 0.9\n",
        "temperature = initial_temperature\n",
        "previous_oobe = None\n",
        "\n",
        "for iteration in range(1, num_iterations + 1):\n",
        "    print(f\"\\n--- Iteration {iteration} ---\")\n",
        "\n",
        "    # Step 4.1: Assign pseudo-labels to unlabeled data\n",
        "    pseudo_labeled_data = rf_model.transform(unlabeled_data).select(\"reduced_features\", \"prediction\")\n",
        "\n",
        "    # Rename and cast columns to match the labeled data schema\n",
        "    pseudo_labeled_data = (\n",
        "        pseudo_labeled_data\n",
        "        .withColumnRenamed(\"prediction\", \"class_index\")            # Rename 'prediction' to 'class_index'\n",
        "        .withColumn(\"class_index\", col(\"class_index\").cast(\"int\"))  # Ensure class_index is INT\n",
        "        .select(\"class_index\", \"reduced_features\")  # Reorder columns to match labeled_data\n",
        "    )\n",
        "\n",
        "    # Cache pseudo-labeled data\n",
        "    pseudo_labeled_data = pseudo_labeled_data.cache()\n",
        "    pseudo_labeled_data.count()  # Trigger caching\n",
        "\n",
        "    # Step 4.2: Combine labeled and pseudo-labeled data\n",
        "    combined_data = labeled_data.union(pseudo_labeled_data).checkpoint()\n",
        "\n",
        "    # Step 4.3: Train new Random Forest with combined data\n",
        "    rf_model = rf.fit(combined_data)\n",
        "\n",
        "    # Step 4.4: Evaluate Out-Of-Bag Error (OOBE) on labeled data\n",
        "    predictions = rf_model.transform(labeled_data)\n",
        "    accuracy, precision, recall = calculate_metrics(predictions)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "    # Monitor OOBE to decide whether to continue\n",
        "    current_oobe = 1 - accuracy  # OOBE is 1 - accuracy\n",
        "    if previous_oobe is not None and current_oobe > previous_oobe:\n",
        "        print(\"OOBE increased. Stopping training and reverting to the previous model.\")\n",
        "        break\n",
        "    previous_oobe = current_oobe\n",
        "\n",
        "    # Step 4.5: Reduce temperature\n",
        "    temperature *= cooling_rate\n",
        "\n",
        "    # Unpersist pseudo-labeled data from the previous iteration\n",
        "    pseudo_labeled_data.unpersist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPUzkQg9zqK"
      },
      "source": [
        "Experiment 2: multiple splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EhNxdgF9zqK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Parameters for iterative training\n",
        "num_iterations = 10\n",
        "initial_temperature = 1.0\n",
        "cooling_rate = 0.9\n",
        "temperature = initial_temperature\n",
        "previous_oobe = None\n",
        "\n",
        "# Step 1: Initialize Random Forest model\n",
        "rf = RandomForestClassifier(featuresCol=\"reduced_features\", labelCol=\"class_index\", numTrees=100, maxDepth=10, seed=42)\n",
        "\n",
        "# Step 2: Split data into labeled and unlabeled subsets\n",
        "labeled_data = df.sample(withReplacement=False, fraction=0.25, seed=42)\n",
        "unlabeled_data = df.subtract(labeled_data).cache()\n",
        "unlabeled_initial = unlabeled_data.sample(withReplacement=False, fraction=0.33, seed=42)  # 25% of the total\n",
        "gradual_unlabeled_remaining = unlabeled_data.subtract(unlabeled_initial).cache()  # Remaining 50%\n",
        "\n",
        "for iteration in range(1, num_iterations + 1):\n",
        "    print(f\"\\n--- Iteration {iteration} ---\")\n",
        "\n",
        "    # Step 3: Train the Random Forest model on labeled data\n",
        "    rf_model = rf.fit(labeled_data)\n",
        "\n",
        "    # Step 4: Predict pseudo-labels for the current chunk of unlabeled data\n",
        "    pseudo_labeled_initial = rf_model.transform(unlabeled_initial).select(\"reduced_features\", \"prediction\")\n",
        "\n",
        "    # Rename and cast columns to match the labeled data schema\n",
        "    pseudo_labeled_initial = (\n",
        "        pseudo_labeled_initial\n",
        "        .withColumnRenamed(\"prediction\", \"class_index\")            # Rename 'prediction' to 'class_index'\n",
        "        .withColumn(\"class_index\", col(\"class_index\").cast(\"int\"))  # Ensure class_index is INT\n",
        "        .select(\"class_index\", \"reduced_features\")  # Reorder columns to match labeled_data\n",
        "    )\n",
        "\n",
        "    # Step 5: Combine labeled data with pseudo-labeled data\n",
        "    combined_data = labeled_data.union(pseudo_labeled_initial).checkpoint()\n",
        "\n",
        "    # Step 6: Train the Random Forest model on combined data\n",
        "    rf_model = rf.fit(combined_data)\n",
        "\n",
        "    # Step 7: Evaluate Out-Of-Bag Error (OOBE) on labeled data\n",
        "    predictions = rf_model.transform(labeled_data)\n",
        "    accuracy, precision, recall = calculate_metrics(predictions)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "    # Monitor OOBE to decide whether to continue\n",
        "    current_oobe = 1 - accuracy  # OOBE is 1 - accuracy\n",
        "    print(f\"OOBE: {current_oobe:.4f}\")\n",
        "\n",
        "    if previous_oobe is not None and current_oobe > previous_oobe:\n",
        "        print(\"OOBE increased. Stopping training and reverting to the previous model.\")\n",
        "        break\n",
        "    previous_oobe = current_oobe\n",
        "\n",
        "    # Step 8: Gradually add more unlabeled data for the next iteration\n",
        "    if iteration < num_iterations:\n",
        "        additional_unlabeled = gradual_unlabeled_remaining.sample(withReplacement=False, fraction=0.25, seed=42)\n",
        "        unlabeled_initial = unlabeled_initial.union(additional_unlabeled).cache()\n",
        "\n",
        "    # Step 9: Reduce temperature (if using simulated annealing or similar strategy)\n",
        "    temperature *= cooling_rate\n",
        "\n",
        "    # Unpersist pseudo-labeled data from the previous iteration\n",
        "    pseudo_labeled_initial.unpersist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaGdBySQ9zqK"
      },
      "source": [
        "testing final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1CneCnD9zqK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to calculate confusion matrix and display metrics\n",
        "def calculate_metrics_with_confusion_matrix(predictions):\n",
        "    # Multiclass evaluator for overall accuracy\n",
        "    evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"class_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        "    )\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "    # Collect predictions and labels for confusion matrix\n",
        "    prediction_and_labels = predictions.select(\"prediction\", \"class_index\").rdd.map(tuple)\n",
        "    metrics = MulticlassMetrics(prediction_and_labels)\n",
        "\n",
        "    # Extract precision and recall per label\n",
        "    labels = predictions.select(\"class_index\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    label_metrics = {label: {\"Precision\": metrics.precision(label), \"Recall\": metrics.recall(label)} for label in labels}\n",
        "\n",
        "    # Display confusion matrix\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix)\n",
        "\n",
        "    # Plot confusion matrix using matplotlib\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(pd.DataFrame(confusion_matrix, index=labels, columns=labels),\n",
        "                annot=True, fmt=\".0f\", cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "    # Return overall metrics\n",
        "    return accuracy, label_metrics\n",
        "\n",
        "# Step 10: Testing on the test dataset\n",
        "print(\"\\n--- Testing on Test Dataset ---\")\n",
        "test_predictions = rf_model.transform(test_data)\n",
        "test_accuracy, test_label_metrics = calculate_metrics_with_confusion_matrix(test_predictions)\n",
        "\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"\\nPer Label Metrics:\")\n",
        "for label, metrics in test_label_metrics.items():\n",
        "    print(f\"Label {label}: Precision = {metrics['Precision']:.4f}, Recall = {metrics['Recall']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hALHs0_U9zqK",
        "outputId": "0002fbe5-fb0c-49ed-984a-6f4485d54892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Iteration 1 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/12 11:51:30 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:51:30 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:55:44 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:56:09 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:56:19 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:56:19 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:56:20 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 11:56:29 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 11:56:37 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 11:56:47 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n",
            "24/12/12 11:57:00 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
            "24/12/12 11:57:14 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
            "24/12/12 11:57:29 WARN DAGScheduler: Broadcasting large task binary with size 8.9 MiB\n",
            "24/12/12 11:57:46 WARN DAGScheduler: Broadcasting large task binary with size 9.9 MiB\n",
            "24/12/12 11:58:08 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
            "24/12/12 11:58:26 WARN DAGScheduler: Broadcasting large task binary with size 1144.5 KiB\n",
            "24/12/12 11:58:38 WARN DAGScheduler: Broadcasting large task binary with size 15.8 MiB\n",
            "24/12/12 11:59:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
            "24/12/12 11:59:26 WARN DAGScheduler: Broadcasting large task binary with size 23.5 MiB\n",
            "24/12/12 12:00:26 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
            "24/12/12 12:00:51 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:00:51 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:01:14 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 12:01:39 WARN DAGScheduler: Broadcasting large task binary with size 17.5 MiB\n",
            "24/12/12 12:04:58 WARN DAGScheduler: Broadcasting large task binary with size 1058.3 KiB\n",
            "24/12/12 12:05:40 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
            "24/12/12 12:08:31 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
            "24/12/12 12:09:07 WARN DAGScheduler: Broadcasting large task binary with size 1144.3 KiB\n",
            "24/12/12 12:09:38 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:10:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
            "24/12/12 12:11:18 WARN DAGScheduler: Broadcasting large task binary with size 15.7 MiB\n",
            "24/12/12 12:13:24 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
            "24/12/12 12:14:20 WARN DAGScheduler: Broadcasting large task binary with size 16.5 MiB\n",
            "24/12/12 12:14:32 WARN DAGScheduler: Broadcasting large task binary with size 16.5 MiB\n",
            "24/12/12 12:14:45 WARN DAGScheduler: Broadcasting large task binary with size 16.5 MiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7699, Precision: 0.7702, Recall: 0.7699\n",
            "OOBE: 0.2301\n",
            "\n",
            "--- Iteration 2 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/12 12:14:57 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:15:03 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:15:03 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:15:04 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:15:13 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 12:15:22 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 12:15:32 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n",
            "24/12/12 12:15:45 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
            "24/12/12 12:16:01 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
            "24/12/12 12:16:15 WARN DAGScheduler: Broadcasting large task binary with size 8.9 MiB\n",
            "24/12/12 12:16:32 WARN DAGScheduler: Broadcasting large task binary with size 9.9 MiB\n",
            "24/12/12 12:16:54 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
            "24/12/12 12:17:13 WARN DAGScheduler: Broadcasting large task binary with size 1144.5 KiB\n",
            "24/12/12 12:17:25 WARN DAGScheduler: Broadcasting large task binary with size 15.8 MiB\n",
            "24/12/12 12:17:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
            "24/12/12 12:18:13 WARN DAGScheduler: Broadcasting large task binary with size 23.5 MiB\n",
            "24/12/12 12:19:15 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
            "24/12/12 12:19:39 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:19:39 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 12:20:01 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
            "24/12/12 12:20:24 WARN DAGScheduler: Broadcasting large task binary with size 17.5 MiB\n",
            "24/12/12 12:25:18 WARN DAGScheduler: Broadcasting large task binary with size 1058.4 KiB\n",
            "24/12/12 12:26:34 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
            "24/12/12 12:28:07 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
            "24/12/12 12:29:01 WARN DAGScheduler: Broadcasting large task binary with size 1144.3 KiB\n",
            "24/12/12 12:30:00 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
            "24/12/12 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
            "24/12/12 12:32:53 WARN DAGScheduler: Broadcasting large task binary with size 15.7 MiB\n",
            "24/12/12 12:36:03 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
            "24/12/12 12:37:51 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "24/12/12 12:38:04 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "24/12/12 12:38:16 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7672, Precision: 0.7676, Recall: 0.7672\n",
            "OOBE: 0.2328\n",
            "OOBE increased. Stopping training and reverting to the previous model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n",
            "24/12/12 12:38:31 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "24/12/12 12:38:34 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "24/12/12 12:38:45 ERROR Executor: Exception in task 3.0 in stage 351.0 (TID 75660)\n",
            "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
            "    process()\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
            "    vs = list(itertools.islice(iterator, batch))\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1292, in prepare\n",
            "    verify_func(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n",
            "    verify_value(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1979, in verify_struct\n",
            "    verifier(v)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n",
            "    verify_value(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1995, in verify_default\n",
            "    verify_acceptable_types(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1871, in verify_acceptable_types\n",
            "    raise TypeError(\n",
            "TypeError: field label: DoubleType() can not accept object 1 in type <class 'int'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 3.0 in stage 351.0 (TID 75660) (hop043.orc.gmu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
            "    process()\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
            "    vs = list(itertools.islice(iterator, batch))\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1292, in prepare\n",
            "    verify_func(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n",
            "    verify_value(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1979, in verify_struct\n",
            "    verifier(v)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n",
            "    verify_value(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1995, in verify_default\n",
            "    verify_acceptable_types(obj)\n",
            "  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1871, in verify_acceptable_types\n",
            "    raise TypeError(\n",
            "TypeError: field label: DoubleType() can not accept object 1 in type <class 'int'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "24/12/12 12:38:45 ERROR TaskSetManager: Task 3 in stage 351.0 failed 1 times; aborting job\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 11.0 in stage 351.0 (TID 75668) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 5.0 in stage 351.0 (TID 75662) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 6.0 in stage 351.0 (TID 75663) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 0.0 in stage 351.0 (TID 75657) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 13.0 in stage 351.0 (TID 75670) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 12.0 in stage 351.0 (TID 75669) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 7.0 in stage 351.0 (TID 75664) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 15.0 in stage 351.0 (TID 75672) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 2.0 in stage 351.0 (TID 75659) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 9.0 in stage 351.0 (TID 75666) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 1.0 in stage 351.0 (TID 75658) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 10.0 in stage 351.0 (TID 75667) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 14.0 in stage 351.0 (TID 75671) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 8.0 in stage 351.0 (TID 75665) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 4.0 in stage 351.0 (TID 75661) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n",
            "24/12/12 12:38:45 WARN TaskSetManager: Lost task 16.0 in stage 351.0 (TID 75673) (hop043.orc.gmu.edu executor driver): TaskKilled (Stage cancelled)\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1451.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 351.0 failed 1 times, most recent failure: Lost task 3.0 in stage 351.0 (TID 75660) (hop043.orc.gmu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1292, in prepare\n    verify_func(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1979, in verify_struct\n    verifier(v)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1995, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1871, in verify_acceptable_types\n    raise TypeError(\nTypeError: field label: DoubleType() can not accept object 1 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1292, in prepare\n    verify_func(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1979, in verify_struct\n    verifier(v)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1995, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1871, in verify_acceptable_types\n    raise TypeError(\nTypeError: field label: DoubleType() can not accept object 1 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Step 9: Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     83\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mtransform(testing_data)\n\u001b[0;32m---> 84\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix for Testing Dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix\u001b[38;5;241m.\u001b[39mtoArray())\n",
            "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mcalculate_confusion_matrix\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      9\u001b[0m rdd \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\n\u001b[1;32m     10\u001b[0m metrics \u001b[38;5;241m=\u001b[39m MulticlassMetrics(rdd)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusionMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/mllib/evaluation.py:311\u001b[0m, in \u001b[0;36mMulticlassMetrics.confusionMatrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusionMatrix\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Matrix:\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    Returns confusion matrix: predicted classes are in columns,\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    they are ordered by class label ascending, as in \"labels\".\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusionMatrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/mllib/common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39ma: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/anaconda3/envs/MMD/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1451.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 351.0 failed 1 times, most recent failure: Lost task 3.0 in stage 351.0 (TID 75660) (hop043.orc.gmu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1292, in prepare\n    verify_func(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1979, in verify_struct\n    verifier(v)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1995, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1871, in verify_acceptable_types\n    raise TypeError(\nTypeError: field label: DoubleType() can not accept object 1 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1292, in prepare\n    verify_func(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1979, in verify_struct\n    verifier(v)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2001, in verify\n    verify_value(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1995, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/szele/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1871, in verify_acceptable_types\n    raise TypeError(\nTypeError: field label: DoubleType() can not accept object 1 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Function to calculate confusion matrix\n",
        "def calculate_confusion_matrix(predictions: DataFrame):\n",
        "    rdd = predictions.select(\"prediction\", \"class_index\").rdd\n",
        "    metrics = MulticlassMetrics(rdd)\n",
        "    return metrics.confusionMatrix()\n",
        "\n",
        "# Parameters for iterative training\n",
        "num_iterations = 10\n",
        "initial_temperature = 1.0\n",
        "cooling_rate = 0.9\n",
        "temperature = initial_temperature\n",
        "previous_oobe = None\n",
        "\n",
        "# Step 1: Split the dataset into training and testing sets\n",
        "data = df.sample(withReplacement=False, fraction=1.0, seed=42)  # Shuffle the data\n",
        "testing_data = data.sample(withReplacement=False, fraction=0.20, seed=42)  # 20% for testing\n",
        "training_data = data.subtract(testing_data).cache()\n",
        "\n",
        "# Step 2: Initialize Random Forest model\n",
        "rf = RandomForestClassifier(featuresCol=\"reduced_features\", labelCol=\"class_index\", numTrees=100, maxDepth=10, seed=42)\n",
        "\n",
        "# Step 3: Split training data into labeled and unlabeled subsets\n",
        "labeled_data = training_data.sample(withReplacement=False, fraction=0.25, seed=42)\n",
        "unlabeled_data = training_data.subtract(labeled_data).cache()\n",
        "unlabeled_initial = unlabeled_data.sample(withReplacement=False, fraction=0.33, seed=42)  # 25% of the total\n",
        "gradual_unlabeled_remaining = unlabeled_data.subtract(unlabeled_initial).cache()  # Remaining 50%\n",
        "\n",
        "for iteration in range(1, num_iterations + 1):\n",
        "    print(f\"\\n--- Iteration {iteration} ---\")\n",
        "\n",
        "    # Step 4: Train the Random Forest model on labeled data\n",
        "    rf_model = rf.fit(labeled_data)\n",
        "\n",
        "    # Step 5: Predict pseudo-labels for the current chunk of unlabeled data\n",
        "    pseudo_labeled_initial = rf_model.transform(unlabeled_initial).select(\"reduced_features\", \"prediction\")\n",
        "\n",
        "    # Rename and cast columns to match the labeled data schema\n",
        "    pseudo_labeled_initial = (\n",
        "        pseudo_labeled_initial\n",
        "        .withColumnRenamed(\"prediction\", \"class_index\")            # Rename 'prediction' to 'class_index'\n",
        "        .withColumn(\"class_index\", col(\"class_index\").cast(\"int\"))  # Ensure class_index is INT\n",
        "        .select(\"class_index\", \"reduced_features\")  # Reorder columns to match labeled_data\n",
        "    )\n",
        "\n",
        "    # Step 6: Combine labeled data with pseudo-labeled data\n",
        "    combined_data = labeled_data.union(pseudo_labeled_initial).checkpoint()\n",
        "\n",
        "    # Step 7: Train the Random Forest model on combined data\n",
        "    rf_model = rf.fit(combined_data)\n",
        "\n",
        "    # Step 8: Evaluate Out-Of-Bag Error (OOBE) on labeled data\n",
        "    predictions = rf_model.transform(labeled_data)\n",
        "    accuracy, precision, recall = calculate_metrics(predictions)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "    current_oobe = 1 - accuracy  # OOBE is 1 - accuracy\n",
        "    print(f\"OOBE: {current_oobe:.4f}\")\n",
        "\n",
        "    if previous_oobe is not None and current_oobe > previous_oobe:\n",
        "        print(\"OOBE increased. Stopping training and reverting to the previous model.\")\n",
        "        break\n",
        "    previous_oobe = current_oobe\n",
        "\n",
        "    # Gradually add more unlabeled data for the next iteration\n",
        "    if iteration < num_iterations:\n",
        "        additional_unlabeled = gradual_unlabeled_remaining.sample(withReplacement=False, fraction=0.25, seed=42)\n",
        "        unlabeled_initial = unlabeled_initial.union(additional_unlabeled).cache()\n",
        "\n",
        "    # Reduce temperature (if using simulated annealing or similar strategy)\n",
        "    temperature *= cooling_rate\n",
        "\n",
        "    # Unpersist pseudo-labeled data from the previous iteration\n",
        "    pseudo_labeled_initial.unpersist()\n",
        "\n",
        "# Step 9: Evaluate the model on the test set\n",
        "test_predictions = rf_model.transform(testing_data)\n",
        "confusion_matrix = calculate_confusion_matrix(test_predictions)\n",
        "print(\"Confusion Matrix for Testing Dataset:\")\n",
        "print(confusion_matrix.toArray())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YXGoYeY9zqK",
        "outputId": "6f9b8e58-f78f-4b43-f8c4-2f0656ba2537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema of predictions:\n",
            "root\n",
            " |-- class_index: integer (nullable = true)\n",
            " |-- reduced_features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/12 12:54:34 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "24/12/12 12:56:46 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total count of predictions: 720034\n",
            "Unique class count in predictions: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/12 12:56:50 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "24/12/12 12:56:51 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n",
            "[Stage 409:===================================================>   (16 + 1) / 17]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix for Testing Dataset:\n",
            "[[282384.  77713.]\n",
            " [ 91118. 268819.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, countDistinct\n",
        "\n",
        "# Function to calculate confusion matrix\n",
        "def calculate_confusion_matrix(predictions: DataFrame):\n",
        "    rdd = predictions.select(\"prediction\", \"class_index\").rdd\n",
        "    metrics = MulticlassMetrics(rdd)\n",
        "    return metrics.confusionMatrix()\n",
        "\n",
        "# Function to retrieve confusion matrix from existing predictions with type casting\n",
        "def retrieve_confusion_matrix_with_cast(predictions: DataFrame):\n",
        "    try:\n",
        "        # Print the schema of predictions\n",
        "        print(\"Schema of predictions:\")\n",
        "        predictions.printSchema()\n",
        "\n",
        "        # Print total count and unique class count\n",
        "        total_count = predictions.count()\n",
        "        unique_classes = predictions.select(\"prediction\").agg(countDistinct(\"prediction\")).collect()[0][0]\n",
        "        print(f\"Total count of predictions: {total_count}\")\n",
        "        print(f\"Unique class count in predictions: {unique_classes}\")\n",
        "\n",
        "        # Cast both prediction and class_index columns to DoubleType\n",
        "        predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n",
        "        predictions = predictions.withColumn(\"class_index\", col(\"class_index\").cast(\"double\"))\n",
        "\n",
        "        # Create RDD and calculate confusion matrix\n",
        "        rdd = predictions.select(\"prediction\", \"class_index\").rdd\n",
        "        metrics = MulticlassMetrics(rdd)\n",
        "        return metrics.confusionMatrix()\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving confusion matrix: {e}\")\n",
        "        return None\n",
        "\n",
        "confusion_matrix = retrieve_confusion_matrix_with_cast(test_predictions)\n",
        "print(\"Confusion Matrix for Testing Dataset:\")\n",
        "if confusion_matrix:\n",
        "    print(confusion_matrix.toArray())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MMD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}